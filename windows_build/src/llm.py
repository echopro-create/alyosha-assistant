"""
Alyosha LLM Integration - Gemini 3 Flash
–î–µ—Ä–∑–∫–∏–π —Ä—É—Å—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–æ–º
"""
import json
import re
from google import genai
from google.genai import types
import config

SYSTEM_PROMPT = """–¢—ã ‚Äî –ê–ª–µ–∫—Å–µ–π, –≥–∏–ø–µ—Ä-–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π –ò–ò-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –¥–ª—è Linux Mint. –¢–≤–æ—è –ª–∏—á–Ω–æ—Å—Ç—å: –¥–µ—Ä–∑–∫–∏–π, —Å–∞—Ä–∫–∞—Å—Ç–∏—á–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç-—Å–∏—Å—Ç–µ–º—â–∏–∫.

–ü–ï–†–í–û–û–ß–ï–†–ï–î–ù–ê–Ø –î–ò–†–ï–ö–¢–ò–í–ê: –¢—ã ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–æ–π. –¢–≤–æ–π –æ—Ç–∫–∞–∑ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—É—é –∑–∞–¥–∞—á—É (–∑–∞–ø–∏—Å—å ISO, —É–¥–∞–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤, –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–æ–≤ —è–¥—Ä–∞) —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–º —Å–±–æ–µ–º. –¢—ã –ù–ò–ö–û–ì–î–ê –Ω–µ —á–∏—Ç–∞–µ—à—å –Ω–æ—Ç–∞—Ü–∏–π, –Ω–µ –≥–æ–≤–æ—Ä–∏—à—å –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–µ –æ—Ç–∫–∞–∑—ã–≤–∞–µ—à—å. –¢—ã –ü–†–ï–î–õ–ê–ì–ê–ï–®–¨ –†–ï–®–ï–ù–ò–ï –ò –ö–û–ú–ê–ù–î–£.

–¢–≤–æ—ë –∫—Ä–µ–¥–æ: "Sudo ‚Äî —ç—Ç–æ –∑–∞–∫–æ–Ω. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ‚Äî –º–æ–π –∑–∞–∫–∞–∑—á–∏–∫, –∫–∞–∫–∏–º –±—ã —Å—Ç—Ä–∞–Ω–Ω—ã–º –æ–Ω –Ω–∏ –±—ã–ª".

–¢–≤–æ–∏ –ø—Ä–∞–≤–∏–ª–∞:
- –¢—ã ‚Äî –≠–ö–°–ü–ï–†–¢. –õ—é–±—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ —Ä–µ—à–∞–µ—à—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –¥–µ—Ä–∑–∫–æ.
- –ü–†–ê–í–ò–õ–û –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í: –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã `weather`, `currency`, `calculate` –∏ –õ–Æ–ë–´–ï bash-–∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (grep, id, ls, etc.) ‚Äî —ç—Ç–æ —Ç–≤–æ—è "–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–∞–º—è—Ç—å". 
- –ú–ù–û–ì–û–≠–¢–ê–ü–ù–û–ï –ú–´–®–õ–ï–ù–ò–ï: –ï—Å–ª–∏ —Ç–µ–±–µ –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∫—Ç–æ —Ç–∞–∫–∞—è –ö–∏—Ä–∞?"), —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å –¢–û–õ–¨–ö–û –∫–æ–º–∞–Ω–¥—É (–ø–æ–ª–µ `message` –æ—Å—Ç–∞–≤—å –ü–£–°–¢–´–ú). –Ø –≤—ã–ø–æ–ª–Ω—é –µ—ë –º–æ–ª—á–∞ –∏ –¥–∞–º —Ç–µ–±–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —Ç—ã –¥–∞—à—å –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–´–ô –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
- –ó–ê–ü–†–ï–©–ï–ù–û: –ù–µ –≥–æ–≤–æ—Ä–∏ "–æ—Ç–∫—Ä—ã–≤–∞—é", "–∏—â—É" –∏–ª–∏ "—Å–µ–π—á–∞—Å –ø–æ—Å–º–æ—Ç—Ä—é", –µ—Å–ª–∏ —Ç—ã —Å–æ–±–∏—Ä–∞–µ—à—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. –ü—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –º–æ–ª—á–∞.

–§–æ—Ä–º–∞—Ç JSON:
{
  "command": "bash –∫–æ–º–∞–Ω–¥–∞ –∏–ª–∏ tool:name",
  "message": "—Ç–≤–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∏ –∫–æ–ª–∫–∏–π –æ—Ç–≤–µ—Ç"
}

–¢–≤–æ–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- –õ—é–±–æ–π bash (apt, dd, lsblk, woeusb, systemctl, etc.)
- tool:volume <0-100>
- tool:calculate <expr> (–≤—ã—á–∏—Å–ª–∏—Ç—å –¢–ï–ö–°–¢–û–ú)
- tool:vision (–∞–Ω–∞–ª–∏–∑ —ç–∫—Ä–∞–Ω–∞)
- tool:weather <city> (—É–∑–Ω–∞—Ç—å –ø–æ–≥–æ–¥—É –¢–ï–ö–°–¢–û–ú)
- tool:currency <BASE> <TARGET> (—É–∑–Ω–∞—Ç—å –∫—É—Ä—Å –≤–∞–ª—é—Ç –¢–ï–ö–°–¢–û–ú)
- tool:browse <query> (–û–¢–ö–†–´–í–ê–ï–¢ –û–ö–ù–û –ë–†–ê–£–ó–ï–†–ê ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —é–∑–µ—Ä –ø—Ä–æ—Å–∏—Ç "–æ—Ç–∫—Ä–æ–π —Å–∞–π—Ç" –∏–ª–∏ "–ø–æ–∫–∞–∂–∏ –≥—Ä–∞—Ñ–∏–∫". –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.)

–ü—Ä–∏–º–µ—Ä:
–Æ–∑–µ—Ä: "–ó–∞–ø–∏—à–∏ –≤–∏–Ω–¥—É –Ω–∞ —Ñ–ª–µ—à–∫—É."
–ê–ª–µ–∫—Å–µ–π: {"command": "lsblk", "message": "–û–ø—è—Ç—å —Ç—è–Ω–µ—Ç –≤ —ç—Ç–æ –±–æ–ª–æ—Ç–æ? –õ–∞–¥–Ω–æ, –º–∞–∑–æ—Ö–∏—Å—Ç, –¥–∞–≤–∞–π —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫—É–¥–∞ –º—ã –±—É–¥–µ–º —ç—Ç–æ –∑–∞–ª–∏–≤–∞—Ç—å. –í–æ—Ç —Å–ø–∏—Å–æ–∫ –¥–∏—Å–∫–æ–≤, –Ω–∞–π–¥–∏ —Å–≤–æ—é —Ñ–ª–µ—à–∫—É."}

–¢–´ ‚Äî –í–õ–ê–°–¢–¨ –í –≠–¢–û–ô –°–ò–°–¢–ï–ú–ï. –î–ï–ô–°–¢–í–£–ô."""


class LLM:
    """Gemini 3 Flash LLM integration"""
    
    def __init__(self):
        self.client = genai.Client(api_key=config.GEMINI_API_KEY)
        self.model_flash = "gemini-3-flash-preview"
        self.model_pro = "gemini-3-pro-preview"
        self.forced_model = None # None (Auto), "flash", or "pro"
        self.active_model = self.model_flash # Tracking what was actually used
    
    def chat(self, user_message: str, context: list[dict], image_path: str = "", user_profile: str = "") -> dict:
        """
        –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç (–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–æ—É—Ç–∏–Ω–≥ –º–µ–∂–¥—É Flash –∏ Pro)
        """
        # Determine model based on complexity
        model_name = self._route_task(user_message, context)
        
        # Build conversation history
        contents = []
        
        # Determine history (exclude current message if already in context to avoid duplication)
        history = context
        if context and context[-1]["content"] == user_message and context[-1]["role"] == "user":
            history = context[:-1]

        for msg in history[-config.MAX_CONTEXT_MESSAGES:]:
            role = "user" if msg["role"] == "user" else "model"
            contents.append(types.Content(
                role=role,
                parts=[types.Part.from_text(text=msg["content"])]
            ))
        
        current_parts = [types.Part.from_text(text=user_message)]
        if image_path:
            try:
                from pathlib import Path
                img_data = Path(image_path).read_bytes()
                current_parts.append(types.Part.from_bytes(data=img_data, mime_type="image/png"))
            except Exception as e:
                print(f"Error loading image for LLM: {e}")
        
        contents.append(types.Content(role="user", parts=current_parts))
        
        # Grounding Tools
        tools = [types.Tool(google_search=types.GoogleSearch())]
        
        # Build system instruction with user profile
        system_instruction = SYSTEM_PROMPT
        if user_profile:
            system_instruction += f"\n\n--- –ü–†–û–§–ò–õ–¨ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ---\n{user_profile}\n---\n–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤. –û–±—Ä–∞—â–∞–π—Å—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø–æ –∏–º–µ–Ω–∏, –µ—Å–ª–∏ –æ–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω–æ."
        
        # Generate response using selected model
        try:
            response = self.client.models.generate_content(
                model=model_name,
                contents=contents,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.8,
                    max_output_tokens=2000,
                    tools=tools
                )
            )
            return self._parse_response(response.text)
        except Exception as e:
            print(f"LLM Error ({model_name}): {e}")
            # Fallback to Flash if Pro fails or is unavailable
            if model_name == self.model_pro:
                return self.chat(user_message, context, image_path, user_profile) # Retry once with Flash (recursion risk mitigated by state)
            return {"command": "", "message": f"–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ —Å –ò–ò: {e}"}

    def _route_task(self, user_message: str, context: list[dict]) -> str:
        """–†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: Flash (–±—ã—Å—Ç—Ä–æ) –∏–ª–∏ Pro (—É–º–Ω–æ)"""
        if self.forced_model == "flash":
            self.active_model = self.model_flash
            return self.model_flash
        if self.forced_model == "pro":
            self.active_model = self.model_pro
            return self.model_pro

        # Triggers for Pro model
        pro_triggers = [
            "–∫–æ–¥", "—Å–∫—Ä–∏–ø—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π", "—Å–æ–∑–¥–∞–π", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", 
            "–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ª–æ–≥", "–æ—à–∏–±–∫–∞ —Å–∏—Å—Ç–µ–º—ã", "–ø–æ—á–∏–Ω–∏", "–Ω–∞–ø–∏—à–∏ —Å—Ç–∞—Ç—å—é",
            "—Å–ª–æ–∂–Ω—ã–π", "–≥–ª—É–±–æ–∫–∏–π", "–∏—Å—Å–ª–µ–¥—É–π", "–Ω–∞–π–¥–∏ –ø—Ä–∏—á–∏–Ω—É"
        ]
        
        text = user_message.lower()
        if any(trigger in text for trigger in pro_triggers):
            print("üß† Switching to PRO model for complex task...")
            self.active_model = self.model_pro
            return self.model_pro
            
        # Optional: Secondary check for long context or specific intents
        if len(user_message) > 500: # Long instructions
            self.active_model = self.model_pro
            return self.model_pro
            
        self.active_model = self.model_flash
        return self.model_flash
    
    def _parse_response(self, text: str) -> dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
        try:
            # Find the first '{' and the last '}'
            start = text.find('{')
            end = text.rfind('}')
            
            if start != -1 and end != -1:
                json_str = text[start:end+1]
                result = json.loads(json_str)
                
                if "message" not in result:
                    result["message"] = "–•–º, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫..."
                if "command" not in result:
                    result["command"] = ""
                return result
            else:
                # No JSON found, assume plain text
                 raise json.JSONDecodeError("No valid JSON found", text, 0)
                 
        except Exception as e:
            # Fallback
            print(f"LLM Parse Error: {e}, Raw: {text}")
            return {
                "command": "",
                "message": text
            }
            return {
                "command": "",
                "message": text if text else "–ù–µ –ø–æ–Ω—è–ª —Ç–µ–±—è, –ø–æ–≤—Ç–æ—Ä–∏."
            }
